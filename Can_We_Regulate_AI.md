# Can We Regulate AI Before It Regulates Us?

*A summary of the 2025 study ‚ÄúA systematic review of regulatory strategies and transparency mandates in AI regulation in Europe, the United States, and Canada‚Äù by Sloane & W√ºllhorst.*

---


We hear about artificial intelligence (AI) everywhere now‚Äîhelping doctors diagnose patients, deciding who gets job interviews, flagging social media posts, or even making parole suggestions in court. But one question keeps coming up: **Who is in charge of AI?**

---

##  First Things First: How Are Countries Even Regulating AI?

Governments have taken **three main approaches** when it comes to regulating AI. You can think of them as:

### 1.  Updating Old Rules (Overhauls)  
Instead of creating something new, some lawmakers are simply **fixing old laws** to cover AI. For example, anti-discrimination laws that were written long before AI even existed are now being updated to stop biased algorithms in hiring or lending. A chatbot making a job decision might now fall under the same protections as a biased human manager.

This is like patching a law that says ‚Äúno discrimination‚Äù so it also includes ‚Äúeven if it comes from a machine.‚Äù

### 2. Writing Brand New AI Laws (Novel Regulations)  
Sometimes, AI introduces problems that old laws just can‚Äôt handle. For example, **facial recognition** used by police, or AI that filters resumes in ways no human can explain. So some governments have written **entirely new laws** to deal with these cases.

In the U.S., many cities have banned government use of facial recognition. Some laws require companies to tell you if an AI tool is evaluating your job application. These new rules are popping up fastest at **local or state** levels, where governments are closer to people and can move quicker.

### 3.  Big, All-in-One Frameworks (Omnibus Laws)  
The third approach is much bigger: **omnibus laws** are designed to create a full system of rules that apply to many sectors at once. The best example is the **European Union‚Äôs AI Act**, which took years to develop. It sorts AI systems into different levels of risk‚Äîfrom ‚Äúunacceptable‚Äù to ‚Äúlow‚Äù‚Äîand applies stronger rules for the most dangerous ones.


---

## So What Do These Laws Actually Try to Do?

Here‚Äôs where it gets interesting: beyond just making rules, many of these laws try to make AI systems more **transparent**. That means helping people understand:
- **When AI is being used**
- **How it works**
- **Who is responsible if something goes wrong**

The study found **six common transparency tools** being used in these laws:

---

##  The Six Ways Lawmakers Are Asking for AI Transparency

1. ** Human in the Loop**  
A human must be able to **monitor or override** important AI decisions. For example, if an AI recommends denying someone healthcare, a person should double-check it before it becomes final.

2. ** Assessments**  
These are like **risk reports** done before the AI goes live. Governments or companies ask: Will this tool treat everyone fairly? Could it cause harm?

3. ** Audits**  
This is a checkup after the system is running. It asks: Is the AI doing what it promised to do? Are there signs of discrimination?

4. ** Disclosures**  
People should be told clearly when they‚Äôre interacting with AI‚Äînot a human. For example, a customer service bot must say it‚Äôs a bot.

5. ** Inventories**  
Some laws require organizations to keep **lists of all the AI systems** they‚Äôre using‚Äîespecially if they‚Äôre high-risk or used in public services. This makes it easier to hold them accountable.

6. ** Red Teaming**  
This is like hiring ethical hackers to **stress-test** an AI system before it goes public. Can it be tricked? Can it cause harm? It‚Äôs a newer idea, but becoming more popular‚Äîespecially for large, general-purpose models like ChatGPT.

---

##  What the Study Really Teaches Us

Many AI laws use combinations of these tools. Some require both **risk assessments and audits**, others mix **disclosure with human oversight**. And while some patterns are new, others repeat across many places.

Still, the authors point out that **transparency alone isn‚Äôt enough**. Some laws are vague, symbolic, or just about compliance. Others fail to include the people most affected by these systems‚Äîlike workers, patients, or low-income communities. And big issues like **who controls AI**, **who profits**, or **who gets left behind** often go unaddressed.

---

##  What‚Äôs Missing‚Äîand What Comes Next?

The study ends with a hopeful but serious message: if we want AI to work **for people**, not just for powerful companies, then AI regulation needs to include **more voices**, more **public participation**, and more **global fairness**.

Researchers are encouraged to:
- Study how laws actually work in real life
- Include perspectives from underrepresented regions
- Focus not just on *what* AI does, but *who* it serves (or harms)

In other words: the future of AI regulation isn‚Äôt just legal. It‚Äôs ethical, social, and global.

---

##  Final Thought

AI is moving fast‚Äîbut the good news is, so are efforts to **govern it thoughtfully**. This study helps us understand the early building blocks of AI regulation, and reminds us that the way we shape technology today will shape society tomorrow. Regulation doesn‚Äôt have to kill innovation. It just **asks that innovation answer to us**, not the other way around.

---


## üìö Reference

Sloane, M., & W√ºllhorst, E. (2025). *A systematic review of regulatory strategies and transparency mandates in AI regulation in Europe, the United States, and Canada*. _Data & Policy, 7_, e11. Cambridge University Press. https://doi.org/10.1017/dap.2024.54
 
